{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = [\"Patient loopt wankel en bibbert.\",\n",
    "         \"Patient is moe van traplopen\",\n",
    "        \"Ze fiets elke dag naar de winkel\"]\n",
    "train_labels = ['l2', 'i2', 'i4']\n",
    "\n",
    "test_texts = [\"Patient is wankel en wiebelt.\",\n",
    "         \"Ik ben uitgeput van een rondje op straat.\",\n",
    "        \"De man gaat met de fiets naar zijn werk.\"]\n",
    "test_labels = ['l2', 'i2', 'i4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertje='wietsedv/bert-base-dutch-cased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30000, 768, padding_idx=3)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bertje)\n",
    "bertje_model = BertModel.from_pretrained(bertje)\n",
    "bertje_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['levensverhaal',\n",
       " 'levenswijze',\n",
       " 'lever',\n",
       " 'leverancier',\n",
       " 'leveranciers',\n",
       " 'leverbaar',\n",
       " 'leverde',\n",
       " 'leveren',\n",
       " 'levering',\n",
       " 'levert',\n",
       " 'lezen',\n",
       " 'lezer',\n",
       " 'lezers',\n",
       " 'lezing',\n",
       " 'lezingen',\n",
       " 'li',\n",
       " 'lib',\n",
       " 'libellen',\n",
       " 'liberaal',\n",
       " 'liberale']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenizer.vocab.keys())[15000:15020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding_vector_from_layer(text, bertje_model, verbose=1):\n",
    "    embedding = np.array\n",
    "    tokenized_text = tokenizer(text, return_tensors=\"pt\")\n",
    "    if verbose:\n",
    "        print(type(tokenized_text))\n",
    "        print('tokenized_text',tokenized_text)\n",
    "    bertje_embeddings, _ = bertje_model(**tokenized_text)\n",
    "    if verbose:\n",
    "        print('bertje_embeddings',bertje_embeddings)\n",
    "    hidden_states = bertje_embeddings[0][0]\n",
    "    if verbose:\n",
    "        print('hidden_states',hidden_states)\n",
    "    embedding= np.array(hidden_states.data)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://huggingface.co/transformers/model_doc/bert.html\n",
    "def get_embedding_vector_from_layer(texts, bertje_model, verbose=1):\n",
    "    bert_vectors = []\n",
    "    for text in texts:\n",
    "        bert_vectors.append(get_sentence_embedding_vector_from_layer(text, bertje_model, verbose))\n",
    "        break\n",
    "    return bert_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tokenized_text {'input_ids': tensor([[    1,  5512, 26105, 15177, 22227, 11281,  9529,   132,    13,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "bertje_embeddings tensor([[[ 0.0226,  0.1662, -0.0553,  ...,  0.0046,  0.3408,  0.1251],\n",
      "         [ 0.8116,  0.4269, -0.2955,  ...,  0.1448, -0.1442, -0.3056],\n",
      "         [ 0.4599,  0.9605, -1.1806,  ..., -0.0358, -0.2691,  0.3070],\n",
      "         ...,\n",
      "         [ 0.9056,  0.3693,  0.0430,  ...,  0.1419,  0.2632,  0.3169],\n",
      "         [ 0.9549,  0.8497, -0.1188,  ..., -1.2821,  0.1217,  0.5042],\n",
      "         [ 0.2007,  0.1240, -0.0035,  ...,  0.2183,  0.1006, -0.0155]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "hidden_states tensor([ 2.2616e-02,  1.6616e-01, -5.5318e-02, -2.5050e-01,  2.5351e-01,\n",
      "        -6.1643e-01,  2.8565e-01, -1.7592e-01,  1.9504e-02,  4.3671e-01,\n",
      "         3.0322e-01, -1.0086e+00, -2.0590e-01, -3.1631e-01, -5.7693e-01,\n",
      "         7.1677e-01, -5.6009e-01, -4.6034e-01, -9.7097e-01,  1.1830e+00,\n",
      "         3.8174e-01, -2.1917e-01,  3.4981e-01,  9.0889e-01, -3.0815e-01,\n",
      "        -1.8763e-01,  3.5132e-01,  6.7285e-01, -6.9646e-01, -3.2665e-01,\n",
      "         6.3206e-01,  8.6209e-01,  5.7873e-01, -1.4336e-01, -6.9890e-01,\n",
      "        -4.3031e-01, -7.1798e-02,  4.0097e-02, -1.2527e-01, -9.4372e-01,\n",
      "         4.5409e-01,  4.6861e-03,  1.1531e-01,  5.8386e-01,  1.6876e-01,\n",
      "         6.6144e-01,  3.9601e-01,  5.3591e-02,  2.2686e-02, -2.3780e-01,\n",
      "        -1.9778e-01,  2.2886e-01,  1.5903e+00,  2.6756e-01,  1.6430e+00,\n",
      "        -3.4546e-01, -5.0657e-02,  3.5487e-01,  3.1393e-01,  4.8495e-01,\n",
      "         1.5876e-02, -2.3404e-02,  1.6472e-01,  2.3141e-01, -2.8980e-01,\n",
      "        -1.8277e-01, -2.1580e-01, -2.8084e-01, -3.1371e-01,  4.0623e-01,\n",
      "        -1.7390e-01,  6.8963e-01, -1.5263e-01, -5.2716e-01, -5.2423e-01,\n",
      "        -2.1082e-01,  4.6910e-01, -8.4567e-01,  3.0406e-01, -8.0253e-01,\n",
      "         6.6216e-01, -4.1233e-01, -2.0787e-01,  2.8925e-01, -1.4346e-02,\n",
      "         8.4589e-01,  4.2614e-01, -3.6824e-01, -9.2682e-01, -1.9390e-01,\n",
      "         1.8724e-01,  4.0015e-01, -1.9559e-01, -3.8846e-01, -3.5487e-01,\n",
      "        -1.1462e+00,  1.9864e-01,  1.5972e-01,  6.6233e-01, -9.3492e-02,\n",
      "        -1.1806e-01,  6.2320e-03, -8.7405e-01,  1.0775e-01, -8.5104e-01,\n",
      "         3.1518e-01,  5.4082e-02,  5.0121e-02,  1.5161e-01,  7.6749e-01,\n",
      "        -7.6205e-01, -1.8257e-01, -5.9974e-01, -4.0847e-01,  1.1603e-01,\n",
      "         1.0925e+00,  1.4091e-01,  4.0305e-01, -1.1921e-01, -1.3224e-02,\n",
      "         1.5287e+00, -2.9452e-01,  1.1281e-01,  1.3666e+00,  2.1437e-01,\n",
      "        -2.0095e-01,  8.0522e-03, -3.1310e-01, -7.8762e-01,  2.3066e-01,\n",
      "        -1.1393e+00, -2.6593e-01, -5.2471e-02,  4.7038e-01, -2.1332e-01,\n",
      "         2.8199e-01,  7.4395e-01,  6.0031e-01,  3.1396e-01, -5.0588e-01,\n",
      "         4.1599e-01, -1.9293e-01,  8.4865e-01,  3.1604e-01,  3.0177e-01,\n",
      "         2.8385e-01,  2.0575e-01, -1.3151e+00, -3.3040e-02, -5.4901e-01,\n",
      "         5.3418e-03,  6.2408e-02, -7.5340e-02, -4.2199e-01,  1.1295e+00,\n",
      "         1.3905e+00,  1.1138e+00,  3.8810e-01, -3.3004e-01, -2.6576e-01,\n",
      "         9.6225e-01,  9.3089e-02, -7.8722e-01, -3.6423e-02, -7.2423e-01,\n",
      "         2.3048e-01,  1.7072e-03,  4.1762e-01, -2.9679e-02,  5.0379e-01,\n",
      "        -5.1291e-02,  1.2712e-01,  1.2654e-01,  6.2852e-01, -1.5740e-01,\n",
      "        -1.1313e+00, -5.1757e-01,  7.9661e-01,  6.3135e-02, -2.5165e-02,\n",
      "         7.4386e-01,  4.8783e-01, -1.1630e-01, -2.2962e-01,  2.7433e-01,\n",
      "        -3.8528e-02, -7.9251e-01,  1.8138e-01, -2.8112e-01, -6.8333e-01,\n",
      "         4.4967e-01, -8.1094e-02, -7.5751e-02, -3.3685e-01,  1.4175e-01,\n",
      "        -3.2674e-03, -7.1864e-01, -3.5223e-01, -1.3827e-03, -5.8247e-01,\n",
      "         5.5043e-01,  3.2802e-01,  2.9234e-01,  9.8943e-02, -1.3024e+00,\n",
      "        -6.6715e-01,  1.0907e+00, -5.0355e-01,  2.5512e-01,  4.8548e-01,\n",
      "         1.2834e-01,  7.1996e-02, -6.2177e-01, -3.2643e-01, -6.0537e-02,\n",
      "        -7.5671e-01,  2.0262e-03,  5.3533e-01,  1.1126e-01,  2.9595e-01,\n",
      "         9.3047e-03, -8.8336e-02, -2.9834e-01,  4.4744e-01, -8.5707e-01,\n",
      "         2.4572e-01,  8.9947e-02, -5.9518e-01,  9.8961e-02,  1.2735e-01,\n",
      "        -6.0870e-01,  6.9426e-01,  3.0222e-01,  5.3774e-02, -7.7529e-01,\n",
      "        -9.0300e-02,  1.0635e-01, -3.5870e-01, -2.6376e-01, -3.3161e-01,\n",
      "        -5.5788e-02,  6.5771e-01,  3.7005e-01,  5.0924e-01, -2.9394e-02,\n",
      "        -5.0666e-01, -2.4019e-01, -1.8151e-01, -3.0259e-01, -7.0297e-01,\n",
      "         7.1644e-01,  7.2351e-01, -7.1476e-02,  1.6425e+00,  1.7021e-01,\n",
      "        -7.7135e-01, -1.4980e-01,  2.0248e-02,  6.2601e-01, -7.5501e-01,\n",
      "        -7.0219e-01,  1.1646e-01, -3.9298e-01, -3.3472e-01, -5.0530e-01,\n",
      "        -2.2175e-01, -7.7902e-01, -4.5500e-01,  7.1532e-01, -5.3241e-01,\n",
      "         5.2034e-01, -1.2610e+00,  7.7613e-02,  4.4751e-01, -6.4440e-01,\n",
      "         1.3228e-01, -1.4607e-01, -6.1725e-01, -9.1537e-01, -2.0353e-02,\n",
      "        -3.4261e-01, -1.8405e-01,  4.6982e-01,  9.3904e-03,  2.1876e-01,\n",
      "        -2.5710e-01,  7.5494e-02, -5.1367e-01,  3.3757e-01,  5.2647e-01,\n",
      "         3.0941e-02, -1.1357e+00,  7.9036e-01,  5.3521e-01,  6.6128e-02,\n",
      "        -5.3589e-01, -9.5051e-01, -6.1887e-01, -8.8849e-01,  1.1359e-01,\n",
      "         1.3089e+00,  7.9404e-01, -3.7151e-01, -4.4760e-01,  4.5500e-01,\n",
      "         5.7495e-01, -9.5920e-01,  1.5551e-01,  2.0708e-01,  1.2007e+00,\n",
      "         4.6825e-01,  1.0548e+00,  1.5379e-01, -1.9732e-01, -7.9606e-01,\n",
      "         5.7502e-02,  1.1254e+00,  1.6418e-01, -2.6553e-02, -1.7394e-01,\n",
      "         1.4241e+00, -1.4145e-01,  4.8858e-01, -6.5058e-01, -1.0278e-01,\n",
      "        -2.4948e-01,  7.1579e-02,  2.4650e-01, -2.7647e-01,  1.7804e-01,\n",
      "        -4.8274e-01, -1.8573e-02, -1.0356e-01, -2.2401e-01, -5.4237e-01,\n",
      "         6.0712e-01,  9.2507e-01,  3.8763e-02, -5.4991e-01, -3.5360e-01,\n",
      "        -5.7250e-01,  6.2531e-01, -5.7764e-01,  4.1851e-01, -1.7557e-01,\n",
      "         4.6251e-01, -1.2161e-01,  2.1953e-01,  1.1816e+00, -7.4208e-06,\n",
      "        -1.5560e-02,  8.5703e-01, -3.9181e-01,  4.9670e-01,  3.8277e-01,\n",
      "        -4.1394e-01,  2.6671e-01, -2.3091e-01,  3.7784e-01, -5.5392e-01,\n",
      "        -3.1270e-02, -9.2938e-01,  2.7548e-01, -1.3248e-01, -8.0702e-01,\n",
      "        -2.2075e-01,  5.9863e-01,  5.7721e-01,  6.0368e-01,  3.2591e-01,\n",
      "         5.9057e-02,  7.7869e-01,  1.8003e-04,  8.0374e-01, -1.3954e-01,\n",
      "        -8.6277e-01, -1.0866e+00,  2.2377e-02,  4.4054e-01,  2.3212e-01,\n",
      "        -9.6849e-01,  7.4470e-01,  3.2204e-01, -1.0071e-01, -7.3275e-01,\n",
      "        -1.4099e+00,  2.0506e-01, -9.2993e-02,  2.0992e-01,  4.7969e-02,\n",
      "         5.3181e-02, -3.9873e-02, -6.4492e-01, -5.5732e-01, -4.4918e-01,\n",
      "         6.4329e-01, -4.0473e-01,  3.3234e-01, -2.1965e-01, -6.6965e-01,\n",
      "        -3.3038e-01,  6.7800e-02,  3.3059e-01, -4.1089e-01,  3.5120e-01,\n",
      "         4.2766e-01, -2.2135e-01, -7.2524e-04,  1.0484e+00, -3.6257e-01,\n",
      "        -1.6981e-01,  6.0552e-01, -1.6513e-01, -7.7281e-01, -1.0539e-01,\n",
      "         1.1452e+00,  1.0701e-01,  5.9472e-01, -3.2080e-01,  7.5335e-01,\n",
      "         4.8820e-01, -2.3746e-01, -9.0229e-02, -1.5492e-01,  3.5101e-01,\n",
      "         4.6403e-01,  2.5809e-02,  7.9323e-01, -4.1121e-02, -5.5494e-01,\n",
      "         3.3419e-02,  5.8393e-01, -6.1383e-01,  6.2274e-01,  8.7182e-01,\n",
      "         3.3834e-01,  7.3067e-01, -2.4235e-01, -1.8186e-01, -3.0023e-01,\n",
      "         5.2711e-01, -9.9291e-02, -2.8616e-01, -7.2187e-02, -4.1124e-01,\n",
      "         4.3334e-01, -9.4718e-02,  4.7080e-01, -1.0419e+00, -2.2966e-01,\n",
      "        -1.8042e-01,  6.1372e-03, -4.3431e-01,  6.2523e-03, -9.2332e-01,\n",
      "        -1.4577e+00, -6.2207e-01, -5.3753e-01,  2.4709e-01,  4.8875e-01,\n",
      "        -5.5555e-02,  4.8799e-01,  7.7422e-01, -1.0547e-01, -3.5218e-01,\n",
      "        -7.6510e-02,  2.5088e-01, -5.9484e-01,  5.7649e-01,  4.9007e-01,\n",
      "        -5.5035e-01, -1.8900e-01, -3.1890e-01, -6.2075e-01,  1.6700e-02,\n",
      "         3.1470e-01, -3.1460e-01, -4.2727e-01, -1.9383e-01,  3.7152e-01,\n",
      "        -6.3815e-01,  4.1430e-01,  8.1866e-01,  4.2805e-01, -5.9803e-01,\n",
      "         7.3037e-01, -2.4603e-02,  2.7320e-01,  4.4328e-02,  7.6852e-01,\n",
      "        -1.0525e-01,  8.1473e-01, -5.5986e-01, -5.9056e-01, -2.1992e-01,\n",
      "        -6.4192e-01, -6.2414e-02,  7.5952e-01, -2.5593e-01,  4.3564e-01,\n",
      "        -6.6078e-01, -3.1135e-01,  3.6334e-01, -4.9636e-02, -5.0143e-01,\n",
      "        -1.0183e+00, -1.0661e+00, -1.3876e-01,  1.1293e-01, -1.8326e-01,\n",
      "        -3.9393e-01,  7.5518e-02, -1.0924e+00, -8.0130e-01, -4.2218e-01,\n",
      "        -2.7590e-01, -3.8671e-01, -7.3037e-01, -9.3650e-01,  5.8291e-01,\n",
      "         4.3870e-01,  3.5037e-01, -2.0124e-01, -6.1906e-01, -5.2880e-01,\n",
      "         1.4153e-02,  2.3870e-01, -3.5801e-01,  1.3005e+00, -5.4787e-01,\n",
      "        -1.0790e-02, -8.6661e-01,  2.5575e-01,  8.3832e-02, -1.1164e-01,\n",
      "         3.3890e-01,  1.3366e-02, -9.3827e-02,  5.9782e-01, -7.6957e-01,\n",
      "        -1.7676e-01, -2.4071e-01,  6.8774e-02,  4.7306e-01,  2.9694e-01,\n",
      "         4.1611e-01,  3.7912e-01,  2.6975e-01, -7.6056e-01,  4.2467e-01,\n",
      "        -1.3469e+00,  3.1797e-01,  2.5343e-01, -5.3290e-01,  1.7506e-01,\n",
      "         5.1204e-01, -5.6111e-01,  3.5784e-01, -7.8000e-01, -3.0521e-01,\n",
      "        -5.2077e-02,  1.6435e-01, -4.5272e-01, -1.8393e-01, -3.0340e-01,\n",
      "         7.5054e-01,  2.1618e-01, -1.0450e-01, -5.2303e-01,  5.6656e-01,\n",
      "        -2.4859e-01, -5.0144e-01, -2.1859e-01,  5.6139e-01, -5.6799e-01,\n",
      "        -1.2897e-01,  6.8714e-01,  1.1590e-01, -4.1540e-01,  5.4388e-01,\n",
      "         1.1511e+00, -8.0150e-01, -2.1985e-01,  2.6913e-01, -7.9244e-01,\n",
      "        -8.3345e-01,  2.7877e-01, -2.4615e-01, -7.3645e-01,  4.0376e-01,\n",
      "        -2.8184e-01,  1.2495e+00, -4.7234e-01, -5.3580e-01, -2.6515e-01,\n",
      "         5.5845e-01, -3.7070e-01, -3.4115e-01, -6.6762e-01,  4.2348e-01,\n",
      "        -6.8448e-01,  4.9151e-01,  1.6049e-01, -3.1062e-01, -3.2177e-02,\n",
      "         6.5335e-01,  2.0213e-01,  1.9437e-01,  1.7457e+00, -1.2027e+00,\n",
      "        -2.7100e-01, -1.5206e-01, -4.7419e-01, -1.2380e-01, -1.5972e+00,\n",
      "         7.2953e-02, -7.1509e-01,  2.9645e-01, -1.4732e-01,  5.1858e-01,\n",
      "         2.7216e-01,  6.3205e-01, -4.1655e-01,  1.7021e-01, -3.0611e-02,\n",
      "         2.7020e-01,  2.1393e-01,  8.6794e-01, -1.5487e-01,  8.1640e-01,\n",
      "         6.4664e-01,  2.3189e-01, -1.0890e-01, -2.6205e-01, -6.0477e-02,\n",
      "        -4.2011e-01,  7.6338e-02,  2.4387e-01,  9.0288e-01,  6.5264e-02,\n",
      "         4.7200e-01, -6.7722e-01, -2.8553e-01,  1.1722e-01,  5.5503e-01,\n",
      "         7.0110e-01, -6.4344e-01,  6.6288e-01,  4.5923e-01,  4.1043e-01,\n",
      "        -1.6456e-01,  4.1823e-01,  3.6277e-01, -5.3024e-01, -6.5299e-01,\n",
      "         2.1585e-01, -4.7081e-02,  6.2290e-01, -2.0361e-02,  5.1043e-01,\n",
      "        -4.5345e-01, -1.2530e-03, -9.8330e-01, -8.7620e-02,  3.2330e-01,\n",
      "         2.0020e-01, -6.7984e-01,  2.8440e-01, -1.6881e-01, -1.1022e-01,\n",
      "        -2.4513e-01, -1.7366e-01,  2.0684e-01, -2.5278e-01, -1.5968e-01,\n",
      "        -2.7158e-01, -2.9979e-01,  3.3664e-01, -1.4555e-01, -1.1118e-01,\n",
      "        -4.7769e-01, -1.1820e-01, -1.4281e-01,  1.8698e-01,  2.4629e-01,\n",
      "         2.2379e-01, -6.1671e-01, -4.3686e-01, -4.4506e-01, -8.0213e-01,\n",
      "         2.4960e-02, -4.7444e-01,  6.1171e-01,  3.6559e-01, -8.5436e-01,\n",
      "         7.7441e-01,  1.9991e-01,  5.5870e-02, -3.6805e-01,  8.5387e-02,\n",
      "         5.8607e-01,  6.5494e-01, -9.4934e-01,  4.8450e-01,  1.0340e+00,\n",
      "         1.6951e-01,  7.8276e-01, -2.5737e-01,  1.3070e-01, -2.0475e-01,\n",
      "        -6.2359e-01,  1.6949e-01, -1.7095e-01,  2.2402e-01, -6.8973e-01,\n",
      "        -1.4554e-01,  3.9207e-01, -1.1399e-01, -7.6494e-01,  1.6986e+00,\n",
      "         1.3039e-01, -2.4908e+00,  3.6881e-01,  5.9561e-01, -6.9321e-02,\n",
      "         1.4170e-02,  2.4199e-01,  1.1432e-01, -2.6143e-01,  5.9614e-01,\n",
      "        -1.6397e-01, -2.3636e-01, -3.6968e-01,  6.2149e-01,  5.6116e-01,\n",
      "         2.3895e-01, -9.5068e-01, -7.0878e-01,  1.4542e-01, -3.5376e-01,\n",
      "         6.2288e-02, -3.1138e-01,  3.8981e-01,  6.7391e-02, -4.3787e-01,\n",
      "        -5.1698e-01,  2.0499e-01,  8.3776e-02, -3.1240e-01, -3.0137e-02,\n",
      "        -3.2657e-01, -1.8556e-01, -6.4000e-01, -1.4172e-01,  1.9002e-01,\n",
      "        -5.5077e-01, -2.5855e-01,  4.9497e-01, -5.8530e-01,  1.3160e+01,\n",
      "         9.6748e-01,  4.3758e-02, -5.4360e-01,  1.7554e+00,  4.9773e-01,\n",
      "         4.6103e-03,  3.4082e-01,  1.2506e-01], grad_fn=<SelectBackward>)\n",
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
      "tokenized_text {'input_ids': tensor([[    1,  5512, 26105, 13903, 22227, 11281, 22446,   132,    13,     2]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "bertje_embeddings tensor([[[ 0.1678, -0.3696,  0.8135,  ...,  0.7304,  0.4661, -0.2932],\n",
      "         [ 0.6967, -0.0038,  0.2205,  ...,  0.8715, -0.0311, -0.3709],\n",
      "         [ 0.6262,  0.7205, -0.6034,  ...,  0.4062, -0.0333,  0.1410],\n",
      "         ...,\n",
      "         [ 1.7006, -0.0023,  0.1572,  ...,  0.0377,  0.6294,  0.3697],\n",
      "         [ 0.4620, -0.6114,  0.5934,  ..., -0.8775,  0.9502,  0.4337],\n",
      "         [ 0.4727, -0.5496,  0.7060,  ..., -0.7478,  0.9609,  0.3810]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n",
      "hidden_states tensor([ 1.6782e-01, -3.6955e-01,  8.1349e-01, -9.0417e-02,  2.9753e-01,\n",
      "        -2.1253e-01,  1.8921e-01,  3.7086e-02, -5.7126e-01,  1.8025e-01,\n",
      "         3.6826e-01, -1.3208e+00, -9.4827e-01, -1.4322e-01,  3.5436e-02,\n",
      "         1.7086e-01, -1.0546e+00, -1.4419e-01, -7.5649e-01,  7.3755e-01,\n",
      "         6.0794e-01, -1.9524e-01,  1.0164e-01,  6.8824e-01, -5.4105e-01,\n",
      "        -2.1759e-01,  2.2925e-02,  1.2896e+00, -3.9129e-01,  3.2997e-01,\n",
      "         5.2033e-01,  2.5834e-01,  8.9187e-01, -1.1303e-01, -1.4816e+00,\n",
      "        -5.3195e-01, -3.3557e-01,  1.6246e-03,  3.8974e-01, -5.3188e-01,\n",
      "         6.8485e-01, -9.8055e-01,  3.4736e-01,  7.2958e-01,  1.9256e-01,\n",
      "         3.1343e-01,  3.7490e-01,  5.2455e-01, -4.6023e-01, -2.5257e-01,\n",
      "        -6.0611e-01,  5.6423e-02,  1.1937e+00,  5.6311e-01,  1.5517e+00,\n",
      "         1.1901e-01, -9.1835e-02,  7.8313e-02,  2.0573e-01,  5.6954e-01,\n",
      "         1.3652e-01, -3.5188e-01,  7.1024e-01,  7.7352e-02, -5.7232e-01,\n",
      "        -6.0680e-01,  5.7915e-02, -3.5654e-01,  6.1234e-01, -7.2908e-02,\n",
      "         6.6582e-01,  3.0453e-02, -3.3015e-01, -1.1800e-01,  4.6319e-01,\n",
      "        -6.7745e-01,  4.1925e-01, -7.6235e-01,  1.3748e+00,  3.8867e-02,\n",
      "        -2.7519e-01,  2.9286e-01,  3.3036e-01,  6.3837e-01,  2.3765e-01,\n",
      "         7.3401e-01, -1.2267e-02, -8.8564e-01, -8.0897e-01, -2.3711e-01,\n",
      "         8.2722e-02, -1.2902e-01,  2.5125e-01, -5.8274e-01, -6.5239e-01,\n",
      "        -1.2187e+00, -3.5973e-01, -1.0350e-02,  2.8188e-01,  8.1587e-01,\n",
      "        -6.4163e-01, -9.8235e-01, -9.5679e-01, -4.7903e-01, -7.5002e-01,\n",
      "         3.2146e-01,  6.5307e-01,  3.5597e-01, -3.7633e-01,  3.8495e-02,\n",
      "        -9.9098e-01,  5.0694e-01, -1.3407e+00, -9.1332e-01, -2.0178e-01,\n",
      "         1.0376e+00, -4.6821e-01, -8.1672e-01,  1.4554e-01, -2.8671e-01,\n",
      "         2.1852e+00, -5.3371e-01,  2.3501e-01, -1.4896e-01,  5.4179e-01,\n",
      "        -4.4355e-01, -5.1897e-02, -1.2485e+00, -6.9856e-01,  5.0379e-01,\n",
      "        -2.6352e-01, -4.3187e-01, -1.8460e-01,  2.8898e-01, -1.3991e-01,\n",
      "         1.2310e-01,  2.5916e-01,  5.3898e-01, -4.6238e-02, -5.0151e-01,\n",
      "         7.5379e-01, -5.2435e-01,  4.6858e-01,  1.9291e-01, -3.4795e-03,\n",
      "         5.3185e-01, -1.4481e-01, -6.3593e-01, -6.4834e-01, -4.9268e-01,\n",
      "        -8.4888e-02,  3.9234e-01,  1.6504e-01, -3.9670e-01,  7.8246e-01,\n",
      "         9.4756e-01, -3.1907e-01,  5.3189e-01,  1.7310e-01,  2.1336e-01,\n",
      "         5.9403e-01, -3.2012e-01, -6.0655e-01,  2.8545e-01, -1.3884e+00,\n",
      "         4.4122e-01,  6.2074e-01,  1.4263e-01, -3.6338e-01,  2.1658e-01,\n",
      "        -1.8143e-01, -1.5287e-01,  1.3683e-01,  9.2905e-02, -4.7557e-01,\n",
      "        -8.0730e-01, -5.7462e-01,  8.4959e-01,  4.9504e-01,  1.2377e-01,\n",
      "         5.4573e-01,  8.0946e-02, -1.7489e-01, -1.2029e+00,  6.2407e-01,\n",
      "        -4.8161e-03, -8.8587e-01, -9.3834e-02, -2.3543e-01, -3.5370e-01,\n",
      "         4.9197e-01,  7.2270e-02,  1.2235e-01, -4.2480e-01,  4.9104e-01,\n",
      "         4.5029e-01, -6.1267e-01, -7.4883e-02, -2.8312e-01, -5.3726e-01,\n",
      "         1.7065e-01,  1.3465e-01,  4.9540e-01,  2.8627e-01, -7.7738e-01,\n",
      "        -6.4603e-01,  9.7176e-01, -3.2466e-02,  8.7358e-02,  3.3759e-01,\n",
      "        -6.8917e-02,  9.5431e-02, -8.0649e-01, -4.3723e-01,  1.3703e-01,\n",
      "        -2.9573e-01,  3.8048e-01,  4.8401e-01, -4.6008e-01,  1.1871e-01,\n",
      "         6.0360e-01, -1.4053e-01, -5.9750e-01,  1.8072e-02, -1.1146e+00,\n",
      "         6.4395e-01,  2.4800e-01,  4.7472e-01, -2.4861e-01, -1.1980e-01,\n",
      "         4.5346e-02,  7.2883e-01, -4.2491e-01,  2.9818e-01, -4.6811e-01,\n",
      "        -9.5410e-02,  2.3075e-01,  1.2783e-01,  9.6602e-02,  4.7809e-01,\n",
      "        -8.1279e-01,  1.2673e+00,  7.3208e-01, -3.3073e-01,  3.6313e-01,\n",
      "        -4.1820e-01, -7.9949e-02,  4.7678e-01,  1.4084e-01, -1.0367e-01,\n",
      "         5.7880e-01,  5.8385e-02,  7.2529e-01,  8.0439e-01,  3.5421e-02,\n",
      "        -7.3112e-01, -2.0011e-02,  6.8463e-01,  1.6335e-01, -3.5027e-02,\n",
      "        -4.3094e-01,  9.6769e-01, -5.5270e-01, -1.9923e-01, -4.5504e-01,\n",
      "        -1.6433e-01, -6.3029e-02, -9.2858e-01,  6.1519e-01,  1.4471e-01,\n",
      "         7.5013e-02, -1.0700e+00,  5.2022e-01,  4.6374e-01, -2.1961e-01,\n",
      "         6.1907e-01,  1.2464e-01, -3.2208e-01, -6.3913e-01,  1.7389e-01,\n",
      "        -8.6275e-01,  1.3789e-01, -4.2763e-02,  5.8082e-02,  4.1027e-01,\n",
      "        -1.4172e-01,  4.4200e-01, -5.9031e-01, -2.1918e-02,  1.9889e-01,\n",
      "        -7.3367e-02, -6.8854e-01,  2.5864e-01,  7.4772e-01, -1.2968e-01,\n",
      "         6.2519e-02, -7.0820e-01, -1.7824e-01, -5.5431e-01,  5.0779e-01,\n",
      "         4.7903e-01,  4.2952e-01, -2.5500e-01, -5.4379e-01,  1.0831e-01,\n",
      "         2.8189e-01, -4.5875e-01,  1.4897e-01,  9.2303e-02,  1.4875e+00,\n",
      "         2.5364e-01, -2.7761e-02,  1.4603e-01,  1.3343e-01, -8.3518e-01,\n",
      "         5.5193e-01,  7.2220e-01,  1.0271e-01, -6.1110e-01,  4.6398e-01,\n",
      "         6.1032e-01, -4.2396e-01,  5.7489e-01, -3.3133e-01,  2.5594e-02,\n",
      "         8.0263e-02,  3.0683e-01,  4.4029e-01, -3.5713e-01, -5.9648e-01,\n",
      "        -5.6328e-01, -3.4211e-01, -4.0327e-01, -7.2995e-01, -1.7510e-01,\n",
      "        -2.5590e-01,  1.0113e+00,  2.6014e-01, -5.2399e-01, -5.5748e-01,\n",
      "        -8.5940e-01,  1.1660e+00, -4.1711e-01,  1.1637e+00, -9.6181e-02,\n",
      "         1.0635e+00,  6.8026e-01,  1.3865e-01,  9.3540e-01,  3.9392e-01,\n",
      "        -1.4874e+00,  2.0194e-01, -4.4402e-02,  8.5555e-01, -2.6876e-01,\n",
      "        -6.2549e-01,  6.5628e-02, -5.0706e-01,  7.6892e-01, -3.6712e-01,\n",
      "         9.1712e-02, -7.0745e-01,  4.5970e-01, -9.6524e-02,  2.9088e-02,\n",
      "         5.1616e-01,  5.1245e-01,  2.6144e-02,  1.8163e+00,  2.8566e-02,\n",
      "         5.7952e-01, -2.4625e-01, -2.7015e-01,  7.6461e-01,  4.6564e-02,\n",
      "        -4.3214e-01, -6.8034e-01, -4.4384e-01,  1.0154e-01,  4.2314e-01,\n",
      "        -1.1598e+00, -4.7078e-02, -1.0068e-01,  5.7560e-02,  8.5752e-02,\n",
      "        -1.2503e+00,  4.5421e-02, -3.1553e-01,  2.8356e-02,  3.5029e-01,\n",
      "         4.4455e-02,  9.6046e-02, -5.8298e-01, -6.9325e-01,  5.4722e-02,\n",
      "         8.2493e-01, -4.5922e-01,  6.5400e-01, -2.7051e-01,  1.7775e-01,\n",
      "        -4.2625e-01, -8.2772e-01,  7.1016e-01, -7.2056e-01,  4.7089e-01,\n",
      "         2.7391e-01, -1.5446e-01,  7.9306e-01,  6.5423e-01, -5.1333e-01,\n",
      "        -1.0794e-02,  7.6765e-01, -3.1994e-01, -6.4689e-01,  5.1055e-02,\n",
      "         4.0212e-01,  2.7670e-01,  9.0391e-01,  4.6840e-01, -2.1424e-01,\n",
      "        -2.2883e-01,  1.4164e-02, -1.5976e-01, -1.9425e-01,  1.3717e-02,\n",
      "         3.9263e-01, -3.3991e-01,  6.5019e-01,  8.8391e-01, -8.2532e-01,\n",
      "         6.3964e-02,  1.3937e-01, -7.4387e-01,  3.8764e-01,  1.3427e+00,\n",
      "         5.8046e-01,  5.8691e-01,  5.6221e-03, -1.2486e-01, -1.0998e+00,\n",
      "         2.0978e-01, -5.8773e-01, -6.3467e-01,  1.0110e-01, -6.2281e-01,\n",
      "         4.2751e-02,  2.8409e-02,  8.1798e-01, -6.2135e-01,  1.2997e-01,\n",
      "        -3.2517e-01, -4.6340e-01, -2.2600e-01,  4.9324e-01, -1.2562e-01,\n",
      "        -1.2451e+00, -1.9510e-01,  3.3610e-02,  3.3115e-01,  4.9796e-01,\n",
      "        -3.7052e-01,  8.3137e-02,  6.2966e-01, -4.5477e-01, -8.7296e-02,\n",
      "        -3.0596e-01,  3.0295e-01, -6.1955e-01,  3.0236e-01, -2.7308e-01,\n",
      "        -8.6783e-01, -6.3672e-01,  1.1865e-02, -2.8733e-01, -4.6596e-01,\n",
      "         7.9152e-02, -4.3046e-01, -5.9860e-01, -3.0219e-01,  1.8054e-01,\n",
      "         5.4302e-02,  1.7471e-01,  5.4244e-01,  4.7691e-01, -5.6859e-01,\n",
      "         2.2540e-01, -5.2140e-01,  1.0515e+00, -4.1335e-01,  2.6441e-01,\n",
      "         1.9827e-01,  3.7486e-01, -2.0694e-01, -3.2780e-01, -3.6515e-01,\n",
      "        -4.5648e-01, -4.9158e-01,  6.6695e-01,  2.1016e-01,  1.2627e-01,\n",
      "        -5.3883e-01, -8.2696e-02, -7.2751e-02, -1.3494e-02,  4.4830e-01,\n",
      "        -6.8690e-01, -1.1325e+00,  2.9882e-01,  5.0143e-01, -1.3411e-01,\n",
      "        -6.6696e-01, -2.8620e-01,  1.0150e-01,  3.4690e-01,  2.2907e-02,\n",
      "        -3.9886e-01, -8.9584e-01,  4.7643e-01, -4.3272e-01,  4.7790e-01,\n",
      "         1.0873e-01, -3.6786e-01, -8.5087e-02, -1.3372e+00, -1.4615e-01,\n",
      "        -4.7654e-01,  2.6067e-01, -2.6738e-01,  1.3984e+00, -4.9355e-01,\n",
      "        -5.2642e-01, -9.4517e-01,  6.1320e-02,  5.2718e-01, -9.6192e-01,\n",
      "        -4.1134e-01, -3.5924e-01, -4.0802e-02, -2.1799e-01, -2.8787e-01,\n",
      "         6.0629e-02, -6.7946e-02,  3.1995e-01, -1.4474e-01,  7.6200e-01,\n",
      "         1.1450e-01,  4.9452e-01,  3.1294e-01, -4.6958e-01, -2.0684e-01,\n",
      "        -9.5282e-01,  1.0247e+00, -2.6635e-01,  1.7502e-01,  5.2836e-01,\n",
      "         6.5121e-01, -3.3354e-02, -1.0144e-01, -8.5683e-01, -7.2613e-02,\n",
      "        -4.3898e-01,  4.3291e-01, -3.3906e-01, -1.6140e-01, -8.9215e-02,\n",
      "         2.4752e-01,  5.5944e-01,  2.7314e-01, -1.6429e-01,  4.5471e-01,\n",
      "         1.7806e-01, -7.3404e-01,  3.8903e-01,  2.0716e-01, -8.5092e-01,\n",
      "        -4.1569e-01,  1.0948e-01,  4.5561e-01, -3.3962e-02,  4.4818e-01,\n",
      "         4.9869e-01, -9.2080e-01, -2.3643e-01,  3.9691e-02, -4.5709e-01,\n",
      "        -5.6023e-01, -4.4997e-01,  2.9933e-01, -5.8628e-01, -3.7017e-01,\n",
      "        -1.1429e+00,  1.0360e+00, -4.1581e-01, -4.9697e-01, -8.4999e-01,\n",
      "         7.3144e-01, -4.2205e-01, -1.7846e-01, -4.9841e-01,  1.9400e-01,\n",
      "        -4.4483e-01,  6.7695e-01,  6.4065e-01,  1.3963e-01, -9.6947e-02,\n",
      "         3.0253e-01, -4.1991e-02,  1.4250e-01,  8.7344e-01, -6.7987e-01,\n",
      "        -3.8363e-01, -6.7593e-01, -1.6963e-01,  7.4054e-02, -8.9894e-01,\n",
      "        -1.9166e-02, -7.5999e-01,  1.9691e-01,  7.7799e-02,  4.6816e-01,\n",
      "        -3.5654e-01,  9.6680e-01, -1.6841e-01,  6.4503e-01,  1.0467e+00,\n",
      "        -7.4083e-03,  5.2013e-01,  9.8110e-01, -9.4326e-02,  1.3537e+00,\n",
      "         7.8333e-02, -3.6613e-02, -1.1907e-01, -6.8977e-02, -9.3897e-02,\n",
      "        -5.4651e-01,  3.1551e-01,  1.5983e-01,  2.8384e-01, -1.4061e-01,\n",
      "         4.1887e-01, -7.0212e-01,  1.5252e-03, -9.3537e-02,  1.9008e-01,\n",
      "         3.8337e-02,  7.5809e-01,  1.2169e+00,  6.7701e-01, -5.7956e-02,\n",
      "         7.0529e-02,  1.8287e-01,  1.0622e-01, -1.9466e-01, -9.7844e-01,\n",
      "         7.1243e-01,  2.2688e-01,  2.0706e-01,  1.3999e+00,  3.5863e-01,\n",
      "        -3.7727e-01,  1.6690e-02, -1.1961e+00,  2.3085e-01, -9.4806e-02,\n",
      "        -2.3121e-02, -1.9730e-01, -1.3622e-01, -6.4474e-01, -4.4687e-01,\n",
      "        -1.5277e-01,  4.1613e-01, -7.3929e-02,  1.3019e-01,  1.2062e-01,\n",
      "         6.0906e-01, -1.4958e-01,  4.8835e-01, -1.0194e+00, -7.6762e-02,\n",
      "         3.2554e-02,  4.7592e-01,  6.4696e-01, -6.1680e-02, -1.8745e-01,\n",
      "         3.6266e-01, -3.8149e-02, -6.0005e-01, -8.4398e-01, -8.3662e-01,\n",
      "        -5.8621e-01, -8.5359e-01,  1.4663e-01,  5.6761e-01, -7.6175e-01,\n",
      "         6.3225e-01,  2.6297e-01,  2.1862e-02,  7.4048e-02, -6.5422e-01,\n",
      "         3.3376e-01,  1.2800e+00, -8.1489e-01,  6.4245e-01,  6.1493e-01,\n",
      "         2.2443e-01,  1.1456e+00,  3.5602e-01,  9.3581e-02, -1.8567e-01,\n",
      "        -6.4477e-02, -2.0359e-01,  6.2987e-02,  5.7023e-01, -6.6031e-01,\n",
      "        -9.6719e-02,  4.3060e-01,  2.3857e-01, -9.5026e-01,  7.5313e-01,\n",
      "         1.8740e-02, -2.4351e+00,  8.7025e-02,  3.4690e-01, -1.1838e-01,\n",
      "        -1.0738e-01, -1.5651e-01, -3.2367e-01, -5.4206e-01,  1.3401e+00,\n",
      "         1.1259e-01, -1.0522e+00, -4.5094e-01,  6.5761e-01,  2.1105e-01,\n",
      "         7.0344e-01, -7.8386e-01, -1.4336e+00,  4.8684e-01, -1.4226e-01,\n",
      "        -3.6000e-01, -1.1267e-01, -7.4436e-02, -2.4980e-01, -3.4312e-01,\n",
      "        -2.2857e-01, -6.7302e-01, -4.4992e-01, -5.0126e-01, -3.2571e-02,\n",
      "        -3.1676e-01,  1.7339e-01, -6.4337e-01, -5.1634e-01,  5.0681e-01,\n",
      "        -5.1700e-01,  5.2702e-01,  6.8269e-01, -1.1273e-01,  1.2970e+01,\n",
      "         7.1831e-01,  6.3536e-01,  1.0910e-01,  1.2958e+00,  1.0275e+00,\n",
      "         7.3037e-01,  4.6613e-01, -2.9316e-01], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "bertje_training_vectors = get_embedding_vector_from_layer(train_texts, bertje_model)\n",
    "bertje_test_vectors = get_embedding_vector_from_layer(test_texts, bertje_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.26161405e-02  1.66163564e-01 -5.53178824e-02 -2.50504553e-01\n",
      "  2.53512800e-01 -6.16430938e-01  2.85651296e-01 -1.75916582e-01\n",
      "  1.95039138e-02  4.36707139e-01  3.03218156e-01 -1.00856233e+00\n",
      " -2.05901951e-01 -3.16312671e-01 -5.76927543e-01  7.16767430e-01\n",
      " -5.60092986e-01 -4.60336387e-01 -9.70969737e-01  1.18297851e+00\n",
      "  3.81742597e-01 -2.19168901e-01  3.49814177e-01  9.08892155e-01\n",
      " -3.08154255e-01 -1.87626630e-01  3.51316750e-01  6.72851384e-01\n",
      " -6.96464419e-01 -3.26653898e-01  6.32064521e-01  8.62086594e-01\n",
      "  5.78729749e-01 -1.43359944e-01 -6.98897243e-01 -4.30311471e-01\n",
      " -7.17981905e-02  4.00970727e-02 -1.25266403e-01 -9.43717301e-01\n",
      "  4.54088598e-01  4.68614511e-03  1.15309402e-01  5.83860695e-01\n",
      "  1.68755591e-01  6.61436796e-01  3.96007657e-01  5.35910800e-02\n",
      "  2.26860344e-02 -2.37796813e-01 -1.97778374e-01  2.28860781e-01\n",
      "  1.59032297e+00  2.67556131e-01  1.64299905e+00 -3.45458478e-01\n",
      " -5.06569669e-02  3.54866236e-01  3.13925833e-01  4.84945476e-01\n",
      "  1.58762187e-02 -2.34038047e-02  1.64723098e-01  2.31408298e-01\n",
      " -2.89801240e-01 -1.82766944e-01 -2.15799987e-01 -2.80838788e-01\n",
      " -3.13707113e-01  4.06232178e-01 -1.73899114e-01  6.89628124e-01\n",
      " -1.52626216e-01 -5.27164042e-01 -5.24230123e-01 -2.10822135e-01\n",
      "  4.69101220e-01 -8.45673263e-01  3.04055512e-01 -8.02532136e-01\n",
      "  6.62156582e-01 -4.12327260e-01 -2.07872793e-01  2.89254874e-01\n",
      " -1.43463761e-02  8.45889091e-01  4.26140159e-01 -3.68235618e-01\n",
      " -9.26818788e-01 -1.93895221e-01  1.87244251e-01  4.00146306e-01\n",
      " -1.95588261e-01 -3.88458520e-01 -3.54871035e-01 -1.14619362e+00\n",
      "  1.98641524e-01  1.59720689e-01  6.62325680e-01 -9.34917778e-02\n",
      " -1.18061863e-01  6.23201579e-03 -8.74047160e-01  1.07753128e-01\n",
      " -8.51037562e-01  3.15175951e-01  5.40823229e-02  5.01212627e-02\n",
      "  1.51610598e-01  7.67493188e-01 -7.62051404e-01 -1.82574332e-01\n",
      " -5.99736273e-01 -4.08472538e-01  1.16030358e-01  1.09248805e+00\n",
      "  1.40912548e-01  4.03052419e-01 -1.19210646e-01 -1.32244937e-02\n",
      "  1.52871227e+00 -2.94517785e-01  1.12807490e-01  1.36662829e+00\n",
      "  2.14366287e-01 -2.00953692e-01  8.05219635e-03 -3.13104361e-01\n",
      " -7.87621140e-01  2.30655506e-01 -1.13926435e+00 -2.65931606e-01\n",
      " -5.24709150e-02  4.70383912e-01 -2.13324487e-01  2.81994224e-01\n",
      "  7.43948638e-01  6.00306034e-01  3.13957006e-01 -5.05880535e-01\n",
      "  4.15986091e-01 -1.92927793e-01  8.48653913e-01  3.16036433e-01\n",
      "  3.01765800e-01  2.83854902e-01  2.05752000e-01 -1.31510901e+00\n",
      " -3.30404490e-02 -5.49010932e-01  5.34180552e-03  6.24078102e-02\n",
      " -7.53399804e-02 -4.21987712e-01  1.12949014e+00  1.39050579e+00\n",
      "  1.11375773e+00  3.88101399e-01 -3.30035567e-01 -2.65762061e-01\n",
      "  9.62246835e-01  9.30886120e-02 -7.87220359e-01 -3.64232771e-02\n",
      " -7.24230349e-01  2.30484128e-01  1.70721859e-03  4.17623043e-01\n",
      " -2.96791270e-02  5.03791809e-01 -5.12907505e-02  1.27120286e-01\n",
      "  1.26539752e-01  6.28520548e-01 -1.57401428e-01 -1.13125098e+00\n",
      " -5.17570555e-01  7.96613812e-01  6.31353632e-02 -2.51648948e-02\n",
      "  7.43860364e-01  4.87827063e-01 -1.16296560e-01 -2.29620323e-01\n",
      "  2.74334580e-01 -3.85284051e-02 -7.92506754e-01  1.81378931e-01\n",
      " -2.81121641e-01 -6.83330059e-01  4.49669540e-01 -8.10936391e-02\n",
      " -7.57505074e-02 -3.36851627e-01  1.41748965e-01 -3.26735526e-03\n",
      " -7.18640685e-01 -3.52227628e-01 -1.38273649e-03 -5.82469761e-01\n",
      "  5.50434172e-01  3.28023076e-01  2.92341411e-01  9.89426970e-02\n",
      " -1.30236638e+00 -6.67150617e-01  1.09065425e+00 -5.03548741e-01\n",
      "  2.55123317e-01  4.85483885e-01  1.28339857e-01  7.19959587e-02\n",
      " -6.21765912e-01 -3.26425701e-01 -6.05373271e-02 -7.56713331e-01\n",
      "  2.02615745e-03  5.35328746e-01  1.11255497e-01  2.95948386e-01\n",
      "  9.30465758e-03 -8.83356184e-02 -2.98335940e-01  4.47435081e-01\n",
      " -8.57070327e-01  2.45721757e-01  8.99467096e-02 -5.95183492e-01\n",
      "  9.89608690e-02  1.27349615e-01 -6.08699739e-01  6.94264889e-01\n",
      "  3.02221179e-01  5.37743233e-02 -7.75293708e-01 -9.02995616e-02\n",
      "  1.06351629e-01 -3.58701766e-01 -2.63759524e-01 -3.31613839e-01\n",
      " -5.57883121e-02  6.57713294e-01  3.70051771e-01  5.09243786e-01\n",
      " -2.93941293e-02 -5.06662548e-01 -2.40185261e-01 -1.81513771e-01\n",
      " -3.02589476e-01 -7.02971041e-01  7.16438174e-01  7.23506391e-01\n",
      " -7.14755505e-02  1.64252591e+00  1.70206100e-01 -7.71354020e-01\n",
      " -1.49798900e-01  2.02482976e-02  6.26010478e-01 -7.55010664e-01\n",
      " -7.02194214e-01  1.16460413e-01 -3.92981261e-01 -3.34718049e-01\n",
      " -5.05300164e-01 -2.21753150e-01 -7.79024839e-01 -4.55001056e-01\n",
      "  7.15318620e-01 -5.32407999e-01  5.20344853e-01 -1.26102507e+00\n",
      "  7.76127800e-02  4.47514296e-01 -6.44400835e-01  1.32282436e-01\n",
      " -1.46065563e-01 -6.17246449e-01 -9.15369451e-01 -2.03533694e-02\n",
      " -3.42607647e-01 -1.84046924e-01  4.69824791e-01  9.39044356e-03\n",
      "  2.18759209e-01 -2.57096678e-01  7.54942670e-02 -5.13668239e-01\n",
      "  3.37569773e-01  5.26472211e-01  3.09407189e-02 -1.13569009e+00\n",
      "  7.90355504e-01  5.35213232e-01  6.61282837e-02 -5.35894752e-01\n",
      " -9.50506091e-01 -6.18869603e-01 -8.88494849e-01  1.13589585e-01\n",
      "  1.30892324e+00  7.94036508e-01 -3.71507943e-01 -4.47602093e-01\n",
      "  4.55002159e-01  5.74947417e-01 -9.59201276e-01  1.55509174e-01\n",
      "  2.07077742e-01  1.20071077e+00  4.68247384e-01  1.05479407e+00\n",
      "  1.53785989e-01 -1.97317094e-01 -7.96060681e-01  5.75018637e-02\n",
      "  1.12536240e+00  1.64177388e-01 -2.65532658e-02 -1.73935547e-01\n",
      "  1.42406690e+00 -1.41446769e-01  4.88584608e-01 -6.50578916e-01\n",
      " -1.02782309e-01 -2.49476776e-01  7.15793818e-02  2.46496081e-01\n",
      " -2.76474893e-01  1.78037286e-01 -4.82742161e-01 -1.85733438e-02\n",
      " -1.03560209e-01 -2.24006757e-01 -5.42369902e-01  6.07121706e-01\n",
      "  9.25071001e-01  3.87627780e-02 -5.49908400e-01 -3.53597730e-01\n",
      " -5.72499692e-01  6.25305176e-01 -5.77644348e-01  4.18510139e-01\n",
      " -1.75565675e-01  4.62511450e-01 -1.21612743e-01  2.19533890e-01\n",
      "  1.18162084e+00 -7.42077827e-06 -1.55595541e-02  8.57029200e-01\n",
      " -3.91810000e-01  4.96696472e-01  3.82774711e-01 -4.13939267e-01\n",
      "  2.66706049e-01 -2.30910033e-01  3.77840430e-01 -5.53923130e-01\n",
      " -3.12696546e-02 -9.29376364e-01  2.75477350e-01 -1.32484883e-01\n",
      " -8.07020605e-01 -2.20752478e-01  5.98634183e-01  5.77207506e-01\n",
      "  6.03676558e-01  3.25906247e-01  5.90566471e-02  7.78694391e-01\n",
      "  1.80028379e-04  8.03735971e-01 -1.39538169e-01 -8.62765133e-01\n",
      " -1.08656621e+00  2.23770142e-02  4.40538704e-01  2.32119396e-01\n",
      " -9.68494356e-01  7.44699538e-01  3.22041452e-01 -1.00710884e-01\n",
      " -7.32749939e-01 -1.40994883e+00  2.05055356e-01 -9.29931551e-02\n",
      "  2.09924757e-01  4.79693003e-02  5.31808063e-02 -3.98725010e-02\n",
      " -6.44917905e-01 -5.57319045e-01 -4.49182570e-01  6.43289804e-01\n",
      " -4.04730082e-01  3.32340091e-01 -2.19649479e-01 -6.69652402e-01\n",
      " -3.30379784e-01  6.78000525e-02  3.30590963e-01 -4.10888404e-01\n",
      "  3.51203948e-01  4.27656293e-01 -2.21354425e-01 -7.25236721e-04\n",
      "  1.04836655e+00 -3.62572968e-01 -1.69812009e-01  6.05522811e-01\n",
      " -1.65133968e-01 -7.72813499e-01 -1.05390050e-01  1.14515054e+00\n",
      "  1.07013412e-01  5.94717622e-01 -3.20800900e-01  7.53346562e-01\n",
      "  4.88200456e-01 -2.37462342e-01 -9.02287215e-02 -1.54924974e-01\n",
      "  3.51008475e-01  4.64027405e-01  2.58086920e-02  7.93228507e-01\n",
      " -4.11205851e-02 -5.54936707e-01  3.34187299e-02  5.83926618e-01\n",
      " -6.13831937e-01  6.22740090e-01  8.71823728e-01  3.38344514e-01\n",
      "  7.30666399e-01 -2.42351472e-01 -1.81859210e-01 -3.00231487e-01\n",
      "  5.27105808e-01 -9.92912278e-02 -2.86155701e-01 -7.21869022e-02\n",
      " -4.11239654e-01  4.33336526e-01 -9.47180837e-02  4.70799625e-01\n",
      " -1.04185224e+00 -2.29656845e-01 -1.80417091e-01  6.13720901e-03\n",
      " -4.34313476e-01  6.25232607e-03 -9.23324764e-01 -1.45769858e+00\n",
      " -6.22066796e-01 -5.37526667e-01  2.47092336e-01  4.88748252e-01\n",
      " -5.55547439e-02  4.87992555e-01  7.74216294e-01 -1.05467871e-01\n",
      " -3.52183819e-01 -7.65104741e-02  2.50880718e-01 -5.94843328e-01\n",
      "  5.76489031e-01  4.90072668e-01 -5.50347090e-01 -1.89000964e-01\n",
      " -3.18896681e-01 -6.20745420e-01  1.67003423e-02  3.14699978e-01\n",
      " -3.14595610e-01 -4.27268565e-01 -1.93828821e-01  3.71524572e-01\n",
      " -6.38152778e-01  4.14297402e-01  8.18661094e-01  4.28051800e-01\n",
      " -5.98032594e-01  7.30367482e-01 -2.46031210e-02  2.73200750e-01\n",
      "  4.43279222e-02  7.68522263e-01 -1.05252542e-01  8.14726293e-01\n",
      " -5.59860051e-01 -5.90564072e-01 -2.19919950e-01 -6.41915262e-01\n",
      " -6.24141172e-02  7.59523988e-01 -2.55932182e-01  4.35643792e-01\n",
      " -6.60781503e-01 -3.11346292e-01  3.63342047e-01 -4.96359728e-02\n",
      " -5.01426697e-01 -1.01833892e+00 -1.06614482e+00 -1.38756320e-01\n",
      "  1.12929255e-01 -1.83255166e-01 -3.93934011e-01  7.55178779e-02\n",
      " -1.09237266e+00 -8.01302016e-01 -4.22184169e-01 -2.75901437e-01\n",
      " -3.86709988e-01 -7.30366647e-01 -9.36504543e-01  5.82913458e-01\n",
      "  4.38695699e-01  3.50365341e-01 -2.01240599e-01 -6.19057894e-01\n",
      " -5.28801143e-01  1.41533576e-02  2.38700151e-01 -3.58010381e-01\n",
      "  1.30046904e+00 -5.47866344e-01 -1.07901637e-02 -8.66607070e-01\n",
      "  2.55753160e-01  8.38324726e-02 -1.11635551e-01  3.38904768e-01\n",
      "  1.33662038e-02 -9.38270390e-02  5.97818911e-01 -7.69574702e-01\n",
      " -1.76760927e-01 -2.40713581e-01  6.87744692e-02  4.73057061e-01\n",
      "  2.96942949e-01  4.16108668e-01  3.79120260e-01  2.69753903e-01\n",
      " -7.60563135e-01  4.24674302e-01 -1.34686983e+00  3.17970634e-01\n",
      "  2.53432840e-01 -5.32902956e-01  1.75063640e-01  5.12036264e-01\n",
      " -5.61110914e-01  3.57837766e-01 -7.80001104e-01 -3.05209666e-01\n",
      " -5.20773381e-02  1.64351135e-01 -4.52723682e-01 -1.83933765e-01\n",
      " -3.03397596e-01  7.50537574e-01  2.16182858e-01 -1.04499489e-01\n",
      " -5.23027360e-01  5.66559076e-01 -2.48593643e-01 -5.01437962e-01\n",
      " -2.18588635e-01  5.61392307e-01 -5.67991257e-01 -1.28966793e-01\n",
      "  6.87143803e-01  1.15901761e-01 -4.15400147e-01  5.43876410e-01\n",
      "  1.15113795e+00 -8.01500201e-01 -2.19850168e-01  2.69128442e-01\n",
      " -7.92437851e-01 -8.33451748e-01  2.78769523e-01 -2.46150166e-01\n",
      " -7.36448169e-01  4.03761148e-01 -2.81842530e-01  1.24952006e+00\n",
      " -4.72340226e-01 -5.35800159e-01 -2.65148938e-01  5.58448255e-01\n",
      " -3.70703697e-01 -3.41148615e-01 -6.67616189e-01  4.23476994e-01\n",
      " -6.84477627e-01  4.91513550e-01  1.60488114e-01 -3.10621947e-01\n",
      " -3.21773626e-02  6.53348923e-01  2.02126727e-01  1.94372952e-01\n",
      "  1.74571502e+00 -1.20273352e+00 -2.70998985e-01 -1.52064219e-01\n",
      " -4.74194199e-01 -1.23800404e-01 -1.59722710e+00  7.29534999e-02\n",
      " -7.15085864e-01  2.96454519e-01 -1.47323146e-01  5.18583357e-01\n",
      "  2.72163481e-01  6.32047594e-01 -4.16553795e-01  1.70211434e-01\n",
      " -3.06114368e-02  2.70198613e-01  2.13927627e-01  8.67938995e-01\n",
      " -1.54873073e-01  8.16400111e-01  6.46635711e-01  2.31885850e-01\n",
      " -1.08902082e-01 -2.62047142e-01 -6.04774803e-02 -4.20112133e-01\n",
      "  7.63383061e-02  2.43870735e-01  9.02876794e-01  6.52637929e-02\n",
      "  4.72000480e-01 -6.77224576e-01 -2.85528719e-01  1.17224261e-01\n",
      "  5.55030704e-01  7.01104522e-01 -6.43440127e-01  6.62878335e-01\n",
      "  4.59233373e-01  4.10434842e-01 -1.64557293e-01  4.18233484e-01\n",
      "  3.62771899e-01 -5.30240476e-01 -6.52987599e-01  2.15854377e-01\n",
      " -4.70811911e-02  6.22901142e-01 -2.03613173e-02  5.10433733e-01\n",
      " -4.53449488e-01 -1.25303492e-03 -9.83295500e-01 -8.76198113e-02\n",
      "  3.23304832e-01  2.00202957e-01 -6.79844916e-01  2.84397334e-01\n",
      " -1.68807939e-01 -1.10220209e-01 -2.45134965e-01 -1.73656702e-01\n",
      "  2.06835136e-01 -2.52782792e-01 -1.59676060e-01 -2.71583349e-01\n",
      " -2.99793541e-01  3.36637199e-01 -1.45548612e-01 -1.11179128e-01\n",
      " -4.77691233e-01 -1.18199423e-01 -1.42811656e-01  1.86978072e-01\n",
      "  2.46290416e-01  2.23786414e-01 -6.16706848e-01 -4.36855793e-01\n",
      " -4.45057184e-01 -8.02128315e-01  2.49603540e-02 -4.74439353e-01\n",
      "  6.11709356e-01  3.65588576e-01 -8.54357541e-01  7.74407268e-01\n",
      "  1.99906141e-01  5.58699556e-02 -3.68054003e-01  8.53870586e-02\n",
      "  5.86069405e-01  6.54943347e-01 -9.49344337e-01  4.84496534e-01\n",
      "  1.03402948e+00  1.69513136e-01  7.82755554e-01 -2.57373422e-01\n",
      "  1.30703777e-01 -2.04745352e-01 -6.23588324e-01  1.69491380e-01\n",
      " -1.70947567e-01  2.24022537e-01 -6.89733624e-01 -1.45535052e-01\n",
      "  3.92066926e-01 -1.13990992e-01 -7.64944136e-01  1.69863868e+00\n",
      "  1.30392537e-01 -2.49079061e+00  3.68806154e-01  5.95607698e-01\n",
      " -6.93207234e-02  1.41701875e-02  2.41993994e-01  1.14324413e-01\n",
      " -2.61426628e-01  5.96136928e-01 -1.63966447e-01 -2.36362606e-01\n",
      " -3.69684905e-01  6.21493697e-01  5.61162114e-01  2.38954067e-01\n",
      " -9.50680792e-01 -7.08778262e-01  1.45421714e-01 -3.53757620e-01\n",
      "  6.22883700e-02 -3.11380923e-01  3.89811873e-01  6.73913583e-02\n",
      " -4.37868059e-01 -5.16979992e-01  2.04994455e-01  8.37758929e-02\n",
      " -3.12400252e-01 -3.01372483e-02 -3.26568037e-01 -1.85563222e-01\n",
      " -6.39996886e-01 -1.41721949e-01  1.90016598e-01 -5.50765514e-01\n",
      " -2.58554876e-01  4.94970113e-01 -5.85299969e-01  1.31600990e+01\n",
      "  9.67480659e-01  4.37577963e-02 -5.43602705e-01  1.75535667e+00\n",
      "  4.97725964e-01  4.61030379e-03  3.40816200e-01  1.25059292e-01]\n"
     ]
    }
   ],
   "source": [
    "print(bertje_training_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          i2       0.00      0.00      0.00         1\n",
      "          i4       0.50      1.00      0.67         1\n",
      "          l2       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.67         3\n",
      "   macro avg       0.50      0.67      0.56         3\n",
      "weighted avg       0.50      0.67      0.56         3\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/piek/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "BERT_classifier = LinearSVC(random_state=0, tol=1e-5)\n",
    "BERT_classifier.fit(bertje_training_vectors, train_labels)\n",
    "SVM_predictions = list(BERT_classifier.predict(bertje_test_vectors))\n",
    "predicted_test_scores= BERT_classifier.decision_function(bertje_test_vectors) \n",
    "print(classification_report(test_labels, SVM_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and similarity with BERTje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 0]\n",
      "Cluster  1\n",
      "['Ze fiets elke dag naar de winkel']\n",
      "\n",
      "Cluster  2\n",
      "['Patient loopt wankel en bibbert.', 'Patient is moe van traplopen']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "num_clusters = 2\n",
    "clustering_model = KMeans(n_clusters=num_clusters)\n",
    "clustering_model.fit(bertje_training_vectors)\n",
    "cluster_assignment = clustering_model.labels_\n",
    "\n",
    "print(cluster_assignment)\n",
    "\n",
    "clustered_sentences = [[] for i in range(num_clusters)]\n",
    "for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
    "    clustered_sentences[cluster_id].append(train_texts[sentence_id])\n",
    "\n",
    "for i, cluster in enumerate(clustered_sentences):\n",
    "    print(\"Cluster \", i+1)\n",
    "    print(cluster)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Fietsen lukt nog niet.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Patient is moe van traplopen (Score: 0.6751)\n",
      "Ze fiets elke dag naar de winkel (Score: 0.6608)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Eerste stapjes met lopen.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Ze fiets elke dag naar de winkel (Score: 0.6962)\n",
      "Patient is moe van traplopen (Score: 0.6672)\n",
      "\n",
      "\n",
      "======================\n",
      "\n",
      "\n",
      "Query: Neemt iedere dag de trap.\n",
      "\n",
      "Top 5 most similar sentences in corpus:\n",
      "Ze fiets elke dag naar de winkel (Score: 0.7464)\n",
      "Patient is moe van traplopen (Score: 0.6450)\n"
     ]
    }
   ],
   "source": [
    "# Query sentences:\n",
    "queries = ['Fietsen lukt nog niet.', 'Eerste stapjes met lopen.', 'Neemt iedere dag de trap.']\n",
    "\n",
    "\n",
    "# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity\n",
    "top_k = 2\n",
    "for query in queries:\n",
    "    query_embedding = get_sentence_embedding_vector_from_layer(query, bertje_model, 0)\n",
    "    cos_scores = util.pytorch_cos_sim(query_embedding, bertje_training_vectors)[0]\n",
    "    cos_scores = cos_scores.cpu()\n",
    "\n",
    "    #We use np.argpartition, to only partially sort the top_k results\n",
    "    top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]\n",
    "\n",
    "    print(\"\\n\\n======================\\n\\n\")\n",
    "    print(\"Query:\", query)\n",
    "    print(\"\\nTop 5 most similar sentences in corpus:\")\n",
    "\n",
    "    for idx in top_results[0:top_k]:\n",
    "        print(train_texts[idx].strip(), \"(Score: %.4f)\" % (cos_scores[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "bert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
