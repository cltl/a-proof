{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_df_from_tsv(filename):\n",
    "    \"\"\"\n",
    "    Creates pd.DataFrame from tsv file in conll format.\n",
    "    \"\"\"\n",
    "    # Make pathlib type\n",
    "    filename = Path(filename)\n",
    "\n",
    "    # Get document id and annotator name from filename\n",
    "    doc_id = filename.parent.stem[-4:]\n",
    "    annotator = filename.stem\n",
    "\n",
    "    # Read file\n",
    "    data = []\n",
    "    with open(filename, 'r') as infile:\n",
    "        for row in infile:\n",
    "            # Skip rows starting with #\n",
    "            if row.startswith('#'):\n",
    "                continue\n",
    "            # Remove '\\n' at end of row\n",
    "            row = row[:-1]\n",
    "            # Split on tab\n",
    "            split_row = row.split('\\t')  \n",
    "            # Skip if empty row\n",
    "            if split_row[0] == '':\n",
    "                continue\n",
    "            # If length is not 5 (some files had only 3 columns), append columns with '_'\n",
    "            if len(split_row) != 5:\n",
    "                split_row.extend(['_', '_'])\n",
    "                \n",
    "            data.append(split_row)\n",
    "    data \n",
    "    \n",
    "    # Create pd.DataFrame\n",
    "    df = pd.DataFrame(data=data, columns=['sent_token_id', 'char_id', \"token\", f\"labels_{annotator}\", f\"relation_{annotator}\"])\n",
    "    # Add column containing file_id\n",
    "    df = df.assign(file_id=doc_id)\n",
    "    \n",
    "    # Split sent_id and token_id (in sentence) and drop orginal and char_id\n",
    "    df['sent_id'] = df.apply(lambda row: row['sent_token_id'].split('-')[0], axis=1)\n",
    "    df['token_s_id'] = df.apply(lambda row: row['sent_token_id'].split('-')[1], axis=1)\n",
    "    df.drop(['sent_token_id', 'char_id'], axis=1, inplace=True)\n",
    "    \n",
    "    # Create column containing token id (from start of doc)\n",
    "    df.reset_index(drop=False, inplace=True)\n",
    "    df.rename({'index': 'token_d_id'}, axis=1, inplace=True)\n",
    "    \n",
    "    # Create index from file id and token id (from start of doc)\n",
    "    df['doc_token_id'] = df.apply(lambda row: str(row['file_id']) + '_'+ str(row['token_d_id']), axis=1)\n",
    "    df.set_index('doc_token_id', inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, filename in enumerate(Path('./sample_data').glob('**/*.tsv')):\n",
    "    \"\"\"\n",
    "    Creates pd.DataFrame by joining files from different annotators and different documents to one\n",
    "    large df\n",
    "    \"\"\"\n",
    "    # Extract annotator name from doc\n",
    "    annotator = filename.stem\n",
    "    \n",
    "    # Use the first file to create df\n",
    "    if index == 0:\n",
    "        df = open_df_from_tsv(filename)\n",
    "        \n",
    "    # Update df with new files\n",
    "    else: \n",
    "        # Create temporary df\n",
    "        df_temp = open_df_from_tsv(filename)\n",
    "    \n",
    "        # if file is already in rows, and annotator is already in colmumns, then update\n",
    "        if df_temp['file_id'][1] in set(df['file_id']) and f'labels_{annotator}' in df.columns:\n",
    "            df.update(df_temp)\n",
    "        # Elif file is in rows (and annotator not yet in columns), then concat with axis=1\n",
    "        elif df_temp['file_id'][1] in set(df['file_id']):\n",
    "            df_temp.drop(['token_d_id', 'token', 'file_id', 'sent_id', 'token_s_id'], axis=1, inplace=True)\n",
    "            df = pd.concat([df, df_temp], axis=1, sort=False)\n",
    "        # Else\n",
    "        else:\n",
    "            df = pd.concat([df, df_temp], join='inner')\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(127, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_d_id</th>\n",
       "      <th>token</th>\n",
       "      <th>labels_avelli</th>\n",
       "      <th>relation_avelli</th>\n",
       "      <th>file_id</th>\n",
       "      <th>sent_id</th>\n",
       "      <th>token_s_id</th>\n",
       "      <th>labels_bos</th>\n",
       "      <th>relation_bos</th>\n",
       "      <th>labels_meskers</th>\n",
       "      <th>relation_meskers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2503_0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NF</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>2503</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503_1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>:</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>2503</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503_2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>Pijn</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>2503</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503_3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>in</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>2503</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2503_4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>de</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>2503</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "      <td>_</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        token_d_id token labels_avelli relation_avelli file_id sent_id  \\\n",
       "2503_0         0.0    NF             _               _    2503       1   \n",
       "2503_1         1.0     :             _               _    2503       1   \n",
       "2503_2         2.0  Pijn             _               _    2503       1   \n",
       "2503_3         3.0    in             _               _    2503       1   \n",
       "2503_4         4.0    de             _               _    2503       1   \n",
       "\n",
       "       token_s_id labels_bos relation_bos labels_meskers relation_meskers  \n",
       "2503_0          1          _            _              _                _  \n",
       "2503_1          2          _            _              _                _  \n",
       "2503_2          3          _            _              _                _  \n",
       "2503_3          4          _            _              _                _  \n",
       "2503_4          5          _            _              _                _  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to pickle\n",
    "df.to_pickle('./sample_data/token_level_df_all_annotators_all_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To read file\n",
    "# df = pd.read_pickle('./sample_data/token_level_df_all_annotators_all_docs.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
