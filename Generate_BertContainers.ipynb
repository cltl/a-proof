{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file path\n",
    "filepath = \"./a-proof/sample_data/INCEpTION_output/Avelli+wk_project_2020-07-24_1202/annotation/notities_2017_deel1_cleaned.csv---2503.conll/avelli.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertContainer:\n",
    "    def __init__(self, key, sen_id, sen, encoding):\n",
    "        self.key = key\n",
    "        self.sen_id = sen_id\n",
    "        self.sen = sen\n",
    "        self.encoding = encoding\n",
    "        \n",
    "        self.annot = []\n",
    "\n",
    "    \n",
    "    def add_anno(self, anno):\n",
    "        self.annot.append(anno)\n",
    "        \n",
    "    def print_container(self):\n",
    "        info = []\n",
    "        print(self.key)\n",
    "        print(self.sen_id)\n",
    "        print(self.sen)\n",
    "        for anno in self.annot:\n",
    "            anno.print_annotation()\n",
    "        print(self.encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation:\n",
    "    def __init__(self, tokens, label):\n",
    "        self.tokens = tokens\n",
    "        self.label = label\n",
    "\n",
    "    def print_annotation(self):\n",
    "        print(self.tokens)\n",
    "        print(self.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "key = 'Notities_VUmc_2017'\n",
    "sen_id = 17\n",
    "sen = 'Patiënt loopt moeilijk'\n",
    "encoding = 1880983\n",
    "tokens1 =  [(\"t1\", \"Patient\"),(\"t2\", \"loopt\")]\n",
    "label1 = 'Lopen'\n",
    "\n",
    "tokens2 =  [(\"t3\", \"moeilijk\")]\n",
    "label2 = 'Stemming'\n",
    "\n",
    "anno1 = Annotation(tokens=tokens1, label=label1)\n",
    "anno2 = Annotation(tokens2, label2)\n",
    "\n",
    " \n",
    "instance = BertContainer(key, sen_id, sen, encoding)\n",
    "instance.add_anno(anno1)\n",
    "instance.add_anno(anno2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notities_VUmc_2017\n",
      "17\n",
      "Patiënt loopt moeilijk\n",
      "[('t1', 'Patient'), ('t2', 'loopt')]\n",
      "Lopen\n",
      "[('t3', 'moeilijk')]\n",
      "Stemming\n",
      "1880983\n"
     ]
    }
   ],
   "source": [
    "instance.print_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv(filepath):\n",
    "    \"\"\"\n",
    "    Reads tsv file. Skips lines starting with '#' (except for '#Text=') and empty lines.\n",
    "    :param filepath: filepath to tsv file\n",
    "    :return: data in list of list.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r') as infile:\n",
    "        data = []\n",
    "        for line in infile:\n",
    "            # Remove unnecessary lines\n",
    "            if line.startswith('#') and not line.startswith('#Text='):\n",
    "                continue\n",
    "            if line.startswith('\\t'):\n",
    "                continue\n",
    "            # Remove '\\n' at end of line\n",
    "            line = line[:-1]\n",
    "            # Split line on tab\n",
    "            line = line.split('\\t')\n",
    "\n",
    "            data.append(line)\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_sentence_lvl(data):\n",
    "    \"\"\"\n",
    "    Creates list of lists on sentence level. Each element of resulting list is a list where the first element\n",
    "    is a str of the sentence. The second element is a list of the rows of tokens. \n",
    "    :param data: List of list with sentence elements starting with '#Text='\n",
    "    :return: List of list. Text separated by sentence.\n",
    "    \"\"\"\n",
    "    text_list = []\n",
    "\n",
    "    for index, line_list in enumerate(data):\n",
    "        # If line is whole sentence\n",
    "        if line_list[0].startswith('#'):\n",
    "            # If not the first sentence\n",
    "            if index != 0:\n",
    "                # Add info from previous sentence\n",
    "                text_list.append(sentence_list)\n",
    "            # Create empty sentence list and append string\n",
    "            sentence_list = []\n",
    "            sentence_list.append(line_list[0][6:])\n",
    "        # If line is last in text\n",
    "        elif index == len(data) - 1:\n",
    "            sentence_list.append(line_list)\n",
    "            text_list.append(sentence_list)\n",
    "\n",
    "        # Else append token level info\n",
    "        else:\n",
    "            sentence_list.append(line_list)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "def get_labels_tokens(sentence_obj):\n",
    "    \"\"\"\n",
    "    Collects tokens related to same label. First loops through all tokens in sentence to collect all labels in a set. \n",
    "    Then loops through labels and through all tokens in a sentence to gather the tokens with that label. \n",
    "    This code could be made more efficient.\n",
    "    :param sentence: List containing rows which belong to a single sentence.\n",
    "    :return: dictionary of labels with matching tokens {'label_id': [('t1', 'token_1'), ('t2', 'token_2')], ...}\n",
    "    \"\"\"\n",
    "    # Define set to collect labels in this sentence\n",
    "    label_set = set()\n",
    "    # For token in sentence\n",
    "    for index, row in enumerate(sentence_obj):\n",
    "        # Continue only if row contains single token (so is not the full sentence) and has a label\n",
    "        if type(row) == list and row[3] != '_':\n",
    "            # Split if there are multiple labels for token. If-statement is for bug fix\n",
    "            if type(row[3]) == str:\n",
    "                row[3] = row[3].split('|')\n",
    "            # Add label to set\n",
    "            label_set.update(row[3])\n",
    "    \n",
    "    # Create dictionary to match label to tokens\n",
    "    label_token_dict = dict()\n",
    "    # Loop through labels and create dictionary entry\n",
    "    for label in label_set:\n",
    "        label_token_dict[label] = []\n",
    "        # Loop through sentence, if the row has the label, then create token tuple and add to dict\n",
    "        for index, row in enumerate(sentence_obj):\n",
    "            if label in row[3]:\n",
    "                token_tuple = ('t' + str(index), row[2])\n",
    "                label_token_dict[label].append(token_tuple)\n",
    "\n",
    "    return label_token_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_1 = \"NF : Pijn in de rug , verlicht met warme handdoek en pcm .\"\n",
    "sentence_2 = \"Mw was emotioneel , schoonzoon hoort morgen de uitslag van een aantal onderzoeken en mw maakt zich hier veel zorgen om .\"\n",
    "\n",
    "def get_BERTje_encoding(sentence):\n",
    "    # Write function\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notities_xyz\n",
      "1\n",
      "NF : Pijn in de rug , verlicht met warme handdoek en pcm .\n",
      "123148\n",
      "\n",
      "Notities_xyz\n",
      "2\n",
      "Mw was emotioneel , schoonzoon hoort morgen de uitslag van een aantal onderzoeken en mw maakt zich hier veel zorgen om .\n",
      "[('t3', 'emotioneel')]\n",
      ".B152: Stemming\n",
      "[('t19', 'veel'), ('t20', 'zorgen')]\n",
      "STM 0\n",
      "[('t5', 'schoonzoon'), ('t6', 'hoort'), ('t7', 'morgen'), ('t8', 'de'), ('t9', 'uitslag'), ('t10', 'van'), ('t11', 'een'), ('t12', 'aantal'), ('t13', 'onderzoeken')]\n",
      "stm\\_reaction\n",
      "[('t16', 'maakt'), ('t17', 'zich'), ('t18', 'hier'), ('t19', 'veel'), ('t20', 'zorgen')]\n",
      ".B152: Stemming\n",
      "[('t3', 'emotioneel')]\n",
      "STM 1\n",
      "123148\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read in data and get it in correct format\n",
    "data = read_tsv(filepath)\n",
    "text_list = get_sentence_lvl(data)\n",
    "\n",
    "# Extract key and annotator name from file name\n",
    "key = 'Notities_xyz' # Extract this info\n",
    "# Include annotator name in BertContainer()\n",
    "\n",
    "# For every sentence in the text\n",
    "for sentence_obj in text_list:\n",
    "    # Extract sentence, sentence_id and encoding\n",
    "    sen = sentence_obj[0]\n",
    "    sen_id = sentence_obj[1][0].split('-')[0]\n",
    "    encoding = 123148 # get_BERTje_encoding(sen)\n",
    "\n",
    "    # Define BertContainer instance\n",
    "    instance = BertContainer(key, sen_id, sen, encoding)\n",
    "\n",
    "    # Create label dictionary and loop through it\n",
    "    label_dict = get_labels_tokens(sentence_obj)\n",
    "    for label, token_list in label_dict.items():\n",
    "        label_clean = label.split('[')[0]\n",
    "        # Add the labels to the BertContainer instance\n",
    "        anno = Annotation(token_list, label_clean)\n",
    "        instance.add_anno(anno)\n",
    "        \n",
    "    instance.print_container()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
